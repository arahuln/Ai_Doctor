{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9177835,"sourceType":"datasetVersion","datasetId":5546853}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport json\nfrom pathlib import Path","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/ai-medical-chatbot/ai-medical-chatbot.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.rename(columns = {\n\"Description\": \"question\",\n\"Patient\": \"patient_text\",\n\"Doctor\": \"doctor_text\",\n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"out_path = Path(\"/kaggle/working/data/clean_medical.jsonl\")\nout_path.parent.mkdir(parents = True, exist_ok = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(out_path, \"w\", encoding = \"utf-8\") as f:\n    for i, row in df.iterrows():\n        q = str(row[\"question\"]).strip()\n        p = str(row[\"patient_text\"]).strip()\n        d = str(row[\"doctor_text\"]).strip()\n\n        if not (q and d):\n            continue\n    \n        obj = {\n            \"id\": f\"row_{i}\",\n            \"question\": q,\n            \"patient_text\": p,\n            \"doctor_text\": d\n        }\n        f.write(json.dumps(obj, ensure_ascii = False)+ \"\\n\")\n\nprint(\"Saved cleaned dataset to \", out_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Building RAG","metadata":{}},{"cell_type":"code","source":"!pip install fastapi uvicorn chromadb sentence-transformers torch numpy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install langchain-nvidia-ai-endpoints langchain langchain-community","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom pathlib import Path\nimport chromadb\nfrom chromadb.config import Settings\nfrom sentence_transformers import SentenceTransformer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA = Path(\"/kaggle/working/data/clean_medical.jsonl\")\nPERSIST_DIR = \"/kaggle/working/store/chroma\"\nCOLLECTION = \"med_consultations\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = SentenceTransformer(\"BAAI/bge-m3\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"client = chromadb.Client(Settings(persist_directory = PERSIST_DIR))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try :\n    client.delete_collection(COLLECTION)\nexcept:\n    pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"coll = client.create_collection(COLLECTION, metadata = {\"hnsw:space\": \"cosine\"})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ids, texts, metas = [], [], []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(DATA, 'r', encoding = 'utf-8') as f:\n    for line in f:\n        row = json.loads(line)\n        retrieval_text = f\"Patient: {row['patient_text']} \\nDoctor: {row['doctor_text']}\"\n        ids.append(row['id'])\n        texts.append(retrieval_text)\n        metas.append({\n            \"question\": row['question'],\n            'doctor_text': row['doctor_text']\n        })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Encoding\", len(texts), \" consultations....\")\nimport torch\ntorch.cuda.empty_cache()\nembeddings = model.encode(texts, batch_size = 8, device = \"cuda\", show_progress_bar = True, normalize_embediings = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"coll.add(ids = ids, embeddings = embeddings, documents = texts, metadatas = metas)\nclient.persist()\nprint(f\"Indexed {len(texts)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Using optimized method","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer\nimport numpy as np\nimport torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN_2\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\nmodel = SentenceTransformer(model_name, device = \"cuda\", token = hf_token)\nmodel.half()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def chunk_text(text, max_tokens = 256):\n#     tokens = tokenizer.encode(text, truncation = False)\n#     for i in range(0, len(tokens), max_tokens):\n#         yield tokenizer.decode(tokens[i:i+max_tokens])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def chunk_text(text, chunk_size = 512, overlap = 50):\n    chunks = []\n    start = 0\n    while start < len(text):\n        end = start + chunk_size\n        chunks.append(text[start:end])\n        start += chunk_size - overlap","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"/kaggle/working/data/clean_medical.jsonl\", 'r', encoding = 'utf-8') as f:\n    full_text = f.read()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"texts = [full_text]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_text[:100]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chunked_texts = []\nfor t in texts:\n    chunked_texts.extend(chunk_text(t, chunk_size = 512, overlap = 50))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeddings = model.encode(\n    chunked_texts,\n    batch_size = 16,\n    show_progress_bar = True,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## New Approach","metadata":{}},{"cell_type":"code","source":"!pip install pandas faiss-cpu sentence-transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sentence_transformers import SentenceTransformer\nimport faiss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/ai-medical-chatbot/ai-medical-chatbot.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"texts = []\nfor _, row in df.iterrows():\n   if pd.notna(row[\"Description\"]):\n       texts.append(str(row[\"Description\"]))\n   if pd.notna(row[\"Patient\"]):\n       texts.append(str(row[\"Patient\"]))\n   if pd.notna(row[\"Doctor\"]):\n       texts.append(str(row[\"Doctor\"]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"docs = df['Doctor'].tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Total texts extracted:\", len(texts))\nprint(\"Total texts extracted:\", len(docs))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def chunk_text(text, chunk_size=500, overlap=50):\n   words = text.split()\n   chunks = []\n   for i in range(0, len(words), chunk_size - overlap):\n       chunk = \" \".join(words[i:i+chunk_size])\n       chunks.append(chunk)\n   return chunks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chunked_texts = []\nfor t in docs:\n   chunked_texts.extend(chunk_text(t))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Total chunked texts:\", len(chunked_texts))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = SentenceTransformer('all-MiniLM-L6-v2')  # small & fast","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeddings = model.encode(chunked_texts, convert_to_numpy=True, show_progress_bar=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dimension = embeddings.shape[1]  # embedding size\nindex = faiss.IndexFlatL2(dimension)  # L2 distance\nindex.add(embeddings)\nprint(\"FAISS index built with\", index.ntotal, \"documents\") ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def search(query, top_k=10):\n   query_embedding = model.encode([query], convert_to_numpy=True)\n   distances, indices = index.search(query_embedding, top_k)\n   results = []\n   for idx in indices[0]:\n       results.append(chunked_texts[idx])\n   return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"I have pain in my lower back\"\nprint(\"Query:\", query)\nprint(\"Top results:\", search(query))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U langchain-community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T10:38:07.753572Z","iopub.execute_input":"2025-09-01T10:38:07.753965Z","iopub.status.idle":"2025-09-01T10:38:11.238817Z","shell.execute_reply.started":"2025-09-01T10:38:07.753943Z","shell.execute_reply":"2025-09-01T10:38:11.238001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\n# Load CSV\ndf = pd.read_csv(\"/kaggle/input/ai-medical-chatbot/ai-medical-chatbot.csv\")\n# Use doctor responses for embeddings\ndocs = df[\"Doctor\"].tolist()\n# LangChain embedding wrapper\nembedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n# Convert docs to vector store\ndoc_texts = [str(d) for d in docs]\nvectorstore = FAISS.from_texts(doc_texts, embedding_model)\n# Build retriever\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T10:38:21.054266Z","iopub.execute_input":"2025-09-01T10:38:21.055133Z","iopub.status.idle":"2025-09-01T10:42:45.125989Z","shell.execute_reply.started":"2025-09-01T10:38:21.055096Z","shell.execute_reply":"2025-09-01T10:42:45.125357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain, RetrievalQA","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T10:43:39.459182Z","iopub.execute_input":"2025-09-01T10:43:39.459520Z","iopub.status.idle":"2025-09-01T10:43:39.608512Z","shell.execute_reply.started":"2025-09-01T10:43:39.459493Z","shell.execute_reply":"2025-09-01T10:43:39.607845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_prompt = PromptTemplate(\n   template=\"\"\"\nYou are a professional medical doctor providing clear and accurate advice.  \nAnswer the patient’s question based **only on the provided context from other doctors**.  \nIf you don’t know, politely say you don’t know and recommend consulting a healthcare professional.  \n### Context from doctors:\n{context}\n### Patient Question:\n{question}\n### Doctor’s Answer:\n\"\"\",\n   input_variables=[\"context\", \"question\"],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T10:44:12.089389Z","iopub.execute_input":"2025-09-01T10:44:12.089686Z","iopub.status.idle":"2025-09-01T10:44:12.093873Z","shell.execute_reply.started":"2025-09-01T10:44:12.089664Z","shell.execute_reply":"2025-09-01T10:44:12.093001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.chains import RetrievalQA\nqa_chain = RetrievalQA.from_chain_type(\n   llm=llm,\n   retriever=retriever,   # Pass retriever directly\n   chain_type=\"stuff\",\n   return_source_documents=False,  # Optional, if you want sources\n   chain_type_kwargs={\n       \"prompt\": custom_prompt  # Now works correctly\n   }\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T11:09:44.688348Z","iopub.execute_input":"2025-09-01T11:09:44.688652Z","iopub.status.idle":"2025-09-01T11:09:44.693153Z","shell.execute_reply.started":"2025-09-01T11:09:44.688629Z","shell.execute_reply":"2025-09-01T11:09:44.692372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"I have a hair fall problem how to counter it?\"\nresponse = qa_chain.run(query)\nprint(\"AI Doctor:\", response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T11:23:53.720576Z","iopub.execute_input":"2025-09-01T11:23:53.720850Z","iopub.status.idle":"2025-09-01T11:23:57.134903Z","shell.execute_reply.started":"2025-09-01T11:23:53.720832Z","shell.execute_reply":"2025-09-01T11:23:57.134073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n# secret_label = \"your-secret-label\"\nnvidia_model_name = UserSecretsClient().get_secret(\"NVIDIA_MODEL_NAME\")\nnvidia_api_key = UserSecretsClient().get_secret(\"NVIDIA_API_KEY\")\nprint(nvidia_model_name , nvidia_api_key[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T10:36:37.878908Z","iopub.execute_input":"2025-09-01T10:36:37.879722Z","iopub.status.idle":"2025-09-01T10:36:38.351271Z","shell.execute_reply.started":"2025-09-01T10:36:37.879682Z","shell.execute_reply":"2025-09-01T10:36:38.350621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install langchain-nvidia-ai-endpoints","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade langchain langchain-core pydantic langchain-nvidia-ai-endpoints","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_nvidia_ai_endpoints import ChatNVIDIA\n# from langchain\n\nllm = ChatNVIDIA(\n    model = nvidia_model_name,\n    api_key = nvidia_api_key,\n)\n\n# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n\n# llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T10:36:40.923710Z","iopub.execute_input":"2025-09-01T10:36:40.923971Z","iopub.status.idle":"2025-09-01T10:36:41.497087Z","shell.execute_reply.started":"2025-09-01T10:36:40.923951Z","shell.execute_reply":"2025-09-01T10:36:41.496562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}