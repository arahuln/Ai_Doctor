{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12892873,"sourceType":"datasetVersion","datasetId":8157247}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import (\n   AutoTokenizer,\n   AutoModelForCausalLM,\n   Trainer,\n   TrainingArguments,\n   DataCollatorForLanguageModeling,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n# -------------------------\n# 1. Config\n# -------------------------\nMODEL_NAME = \"EleutherAI/gpt-neo-125M\"   # small free model\nMAX_LEN = 256\nBATCH_SIZE = 2\nEPOCHS = 3\nOUTPUT_DIR = \"/kaggle/working/lora-medbot\"\n# -------------------------\n# 2. Load dataset\n# -------------------------\n# Assume you already converted your CSV -> JSONL (train.jsonl / val.jsonl)\ndataset = load_dataset(\"json\", data_files={\"train\": \"/kaggle/input/dataset/train.jsonl\", \"val\": \"/kaggle/input/dataset/val.jsonl\"})\n# Format examples into plain text\ndef format_example(example):\n   instruction = example.get(\"instruction\", \"\")\n   context = example.get(\"input\", \"\")\n   response = example.get(\"output\", \"\")\n   text = f\"Instruction: {instruction}\\nInput: {context}\\nOutput: {response}\"\n   return {\"text\": text}\ndataset = dataset.map(format_example)\n# -------------------------\n# 3. Tokenizer\n# -------------------------\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nif tokenizer.pad_token is None:\n   tokenizer.pad_token = tokenizer.eos_token  # GPT-Neo doesnâ€™t have pad token\ndef tokenize_fn(example):\n   return tokenizer(\n       example[\"text\"],\n       truncation=True,\n       padding=\"max_length\",\n       max_length=MAX_LEN,\n   )\ntokenized_dataset = dataset.map(tokenize_fn, batched=True, remove_columns=dataset[\"train\"].column_names)\n# -------------------------\n# 4. Load base model\n# -------------------------\nmodel = AutoModelForCausalLM.from_pretrained(\n   MODEL_NAME,\n   torch_dtype=torch.float32,\n   device_map=\"auto\"\n)\n# -------------------------\n# 5. LoRA config\n# -------------------------\nlora_config = LoraConfig(\n   r=8,\n   lora_alpha=16,\n   target_modules=[\"q_proj\",\"v_proj\"],  # works for GPT-like models\n   lora_dropout=0.05,\n   bias=\"none\",\n   task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n# -------------------------\n# 6. Trainer setup\n# -------------------------\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\ntraining_args = TrainingArguments(\n   output_dir=OUTPUT_DIR,\n   overwrite_output_dir=True,\n   num_train_epochs=EPOCHS,\n   per_device_train_batch_size=BATCH_SIZE,\n   per_device_eval_batch_size=BATCH_SIZE,\n   save_steps=200,\n   save_total_limit=2,\n   logging_dir=\"./logs\",\n   logging_steps=50,\n   eval_strategy=\"steps\",\n   eval_steps=200,\n   learning_rate=2e-4,\n   warmup_steps=100,\n   weight_decay=0.01,\n   fp16=torch.cuda.is_available(),\n   push_to_hub=False,\n)\ntrainer = Trainer(\n   model=model,\n   args=training_args,\n   train_dataset=tokenized_dataset[\"train\"],\n   eval_dataset=tokenized_dataset[\"val\"],\n   tokenizer=tokenizer,\n   data_collator=data_collator,\n)\n# -------------------------\n# 7. Train\n# -------------------------\ntrainer.train()\n# -------------------------\n# 8. Save model\n# -------------------------\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T06:09:34.815050Z","iopub.execute_input":"2025-08-28T06:09:34.815245Z"}},"outputs":[{"name":"stderr","text":"2025-08-28 06:09:56.654893: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756361397.011340      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756361397.113274      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e09006a86fb44236b35937b861d67ed5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating val split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07979f1335c94d8baf4ff7e1a36da0fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/231224 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80abccf4a3ba4fd59d867e622c2ec346"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25692 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4202cbed3f5648b7b46276a7abc0e671"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2db4fc80f3fe44348fb358802832570a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baef59f7766942ef8ac0a6d153071015"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d39e124e9464b5c94224eeca564f370"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3420a423d9f243bbbb5f38e46a8a1e47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cce5bc20844422ea4e427489fab0360"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/231224 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63f946909b3141dab2d8ab59f6f4513f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25692 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51b27dfcfe384cb081ab66f67b90dc77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcc89b50778240ebb3f6ad7a54f6c4f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca40d4211ee148ed9e2da958a022c57c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7a58ccd8e3d4928bbf1aec8542d3b7e"}},"metadata":{}},{"name":"stdout","text":"trainable params: 294,912 || all params: 125,493,504 || trainable%: 0.2350\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1118726800.py:89: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}